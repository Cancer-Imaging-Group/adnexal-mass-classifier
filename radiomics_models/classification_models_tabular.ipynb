{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc10516-040c-44c2-a4e1-80312eec629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from scipy.stats import sem, t, ttest_1samp\n",
    "from sklearn import svm\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, Lasso, ElasticNet\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.metrics import (accuracy_score, balanced_accuracy_score, f1_score, \n",
    "                             precision_score, recall_score, roc_auc_score, \n",
    "                             confusion_matrix, brier_score_loss, classification_report)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from xgboost import XGBClassifier\n",
    "from lifelines import KaplanMeierFitter\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b40a2a-8c6b-4918-bedf-b562d3c83ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('yourdara.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee4a17b-ded9-4c8c-bbcf-5173a5e4d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[data['centre']!='xxxx']\n",
    "test_data = data[data['centre']=='xxx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a972f0-029f-4d95-9e08-b04349c2aeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mice_imputer = IterativeImputer()\n",
    "\n",
    "# Define a mapping for 'yes' and 'no' strings to binary values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Update the list of categorical columns\n",
    "categorical_cols = ['cat1', 'cat2', 'etc']\n",
    "\n",
    "\n",
    "# Perform encoding for categorical columns in both the training and test sets\n",
    "train_encoded = pd.get_dummies(train_data, columns=categorical_cols)\n",
    "test_encoded = pd.get_dummies(test_data, columns=categorical_cols)\n",
    "\n",
    "# Align the training and testing data to ensure they have the same columns\n",
    "train_aligned, test_aligned = train_encoded.align(test_encoded, join='outer', axis=1, fill_value=0)\n",
    "\n",
    "# Fit the imputer on the training data\n",
    "mice_imputer.fit(train_aligned)\n",
    "\n",
    "# Use the fitted imputer to transform the training data\n",
    "train_imputed = mice_imputer.transform(train_aligned)\n",
    "train_imputed = pd.DataFrame(train_imputed, columns=train_aligned.columns)\n",
    "\n",
    "# Use the same fitted imputer to transform the test data\n",
    "test_imputed = mice_imputer.transform(test_aligned)\n",
    "test_imputed = pd.DataFrame(test_imputed, columns=test_aligned.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c6aaf-426d-4d07-8f5f-ddb0cc19fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# List of continuous columns to standardize\n",
    "continuous_cols = 'list of continuous cols'\n",
    "\n",
    "# Create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the continuous columns in the training data\n",
    "scaler.fit(train_imputed[continuous_cols])\n",
    "\n",
    "# Use the fitted scaler to transform the continuous columns in the training data\n",
    "train_imputed[continuous_cols] = scaler.transform(train_imputed[continuous_cols])\n",
    "\n",
    "# Use the same fitted scaler to transform the continuous columns in the test data\n",
    "test_imputed[continuous_cols] = scaler.transform(test_imputed[continuous_cols])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35869c-7cb0-4399-a149-375767d72347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import ConvergenceWarning\n",
    "ConvergenceWarning('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fedacc5-04e7-4367-a643-a5bef9effa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature and target variables\n",
    "y = train_imputed['Outcome Col']\n",
    "x = train_imputed.drop(columns=[]\n",
    "np.random.seed(123)\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression(penalty='l1', solver='saga')\n",
    "\n",
    "\n",
    "\n",
    "# Grid search for optimal hyperparameters\n",
    "param_grid = {'C': np.logspace(-4, 4, 50)}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='roc_auc')\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "# Fit the model with optimal hyperparameters\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "pred = model.predict(x)\n",
    "\n",
    "# Get the names of the selected features\n",
    "coef = model.coef_[0]\n",
    "Lasso_selected_features = np.array(x.columns)[coef != 0]\n",
    "\n",
    "print(len(Lasso_selected_features))\n",
    "print(Lasso_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24682f0a-fff3-4aac-b5ed-81c2f948ba17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression(penalty='elasticnet', solver='saga')\n",
    "\n",
    "# Grid search for optimal hyperparameters\n",
    "param_grid = {\n",
    "    'C': np.logspace(-4, 4, 50),\n",
    "    'l1_ratio': np.linspace(0, 1, 10)\n",
    "}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='roc_auc')\n",
    "grid_search.fit(x, y)\n",
    "\n",
    "# Fit the model with optimal hyperparameters\n",
    "model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "pred = model.predict(x)\n",
    "\n",
    "# Get the names of the selected features\n",
    "coef = model.coef_[0]\n",
    "Elastic_selected_features = np.array(x.columns)[coef != 0]\n",
    "\n",
    "print(len(Elastic_selected_features))\n",
    "print(Elastic_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805a0de-c78a-49c9-b767-d6a2308be220",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(321)\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Perform Recursive Feature Elimination\n",
    "selector = RFECV(model, step=1, cv=10)\n",
    "selector = selector.fit(x, y_encoded)\n",
    "\n",
    "# Print the results\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)\n",
    "\n",
    "# Get the names of the selected features\n",
    "RFE_selected_features = np.array(x.columns)[selector.support_]\n",
    "\n",
    "print(len(RFE_selected_features))\n",
    "print(RFE_selected_features)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44858e2-fa42-44d2-8ebc-b13798f9caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(x)\n",
    "\n",
    "# Print explained variance ratio\n",
    "var_explained = pca.explained_variance_ratio_\n",
    "cum_var_explained = np.cumsum(var_explained)\n",
    "\n",
    "print(\"Variance explained by each component:\")\n",
    "print(var_explained)\n",
    "\n",
    "print(\"Cumulative variance explained:\")\n",
    "print(cum_var_explained)\n",
    "\n",
    "# Scree plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(range(1, len(var_explained) + 1), var_explained, 'o-')\n",
    "plt.title('Scree plot')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Proportional variance explained')\n",
    "plt.show()\n",
    "\n",
    "# Get the names of the selected features\n",
    "PCA_selected_features = np.array(x.columns)[pca.components_[0] != 0]\n",
    "\n",
    "print(len(PCA_selected_features))\n",
    "print(PCA_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46804979-f454-466b-b624-5086e037882f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the model\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "\n",
    "# Define Boruta feature selection method\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=123)\n",
    "\n",
    "# Find all relevant features\n",
    "feat_selector.fit(x.values, y.values.ravel())\n",
    "\n",
    "# Check selected features\n",
    "Boruta_selected_features = x.columns[feat_selector.support_].tolist()\n",
    "\n",
    "print(len(Boruta_selected_features))\n",
    "print(Boruta_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a41bd-7ae5-4d4c-a9ef-0c6602131c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "# Define the model\n",
    "model = RandomForestClassifier(n_estimators=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.barh(x.columns, importances)\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature importances from Random Forest')\n",
    "plt.show()\n",
    "\n",
    "# Get the names of the selected features\n",
    "RF_selected_features = x.columns[importances > 0]\n",
    "\n",
    "print(len(RF_selected_features))\n",
    "print(RF_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c4bc1-4bd3-4f06-8ac3-8f2ad46c8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Select the 10 best features based on Mutual Information\n",
    "selector = SelectKBest(mutual_info_classif, k=10)\n",
    "selector.fit(x, y)\n",
    "\n",
    "# Get the names of the selected features\n",
    "MIM_selected_features = x.columns[selector.get_support()]\n",
    "\n",
    "print(len(MIM_selected_features))\n",
    "print(MIM_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4cc1c3-651d-4cff-bd94-75563b704ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute Pearson correlation with the outcome\n",
    "pearson_corr = x.corrwith(y).abs()\n",
    "\n",
    "# Select the top 40% of features\n",
    "top_40_percent = int(0.4 * len(pearson_corr))\n",
    "pearson_selected_features = pearson_corr.nlargest(top_40_percent).index\n",
    "\n",
    "print(len(pearson_selected_features))\n",
    "print(pearson_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c976d36-5658-4fed-b15e-ee00f465e273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute Spearman correlation with the outcome\n",
    "spearman_corr = x.corrwith(y, method='spearman').abs()\n",
    "\n",
    "# Select the top 40% of features\n",
    "spearman_selected_features = spearman_corr.nlargest(top_40_percent).index\n",
    "\n",
    "print(len(spearman_selected_features))\n",
    "print(spearman_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a9cc1-23f5-45f6-91f6-1a841d8ec632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate absolute Kendall correlation with the outcome\n",
    "kendall_corr = x.corrwith(y, method='kendall').abs()\n",
    "\n",
    "# Select the top 40% of features\n",
    "kendall_selected_features = kendall_corr.nlargest(top_40_percent).index\n",
    "\n",
    "print(len(kendall_selected_features))\n",
    "print(kendall_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f436a750-c00b-4b98-b591-23f0c637f142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select the 10 best features based on ANOVA F-value\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "selector.fit(x, y)\n",
    "\n",
    "# Get the names of the selected features\n",
    "fvalue_selected_features = x.columns[selector.get_support()]\n",
    "\n",
    "print(len(fvalue_selected_features))\n",
    "print(fvalue_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e0686d-d15a-4b01-b5dc-5cfdbe693bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Remove all features with zero variance\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(x)\n",
    "\n",
    "# Get the names of the selected features\n",
    "variance_selected_features = x.columns[selector.get_support()]\n",
    "\n",
    "print(len(variance_selected_features))\n",
    "print(variance_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22853d-590c-43f6-8479-ede9c6295de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Sequential Forward Selection\n",
    "sfs = SFS(LogisticRegression(),\n",
    "           k_features=10,\n",
    "           forward=True,\n",
    "           floating=False,\n",
    "           scoring='accuracy',\n",
    "           cv=0)\n",
    "\n",
    "sfs = sfs.fit(x, y)\n",
    "\n",
    "# Get the names of the selected features\n",
    "sfs_selected_features = list(sfs.k_feature_names_)\n",
    "\n",
    "print(len(sfs_selected_features))\n",
    "print(sfs_selected_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d337b-e28a-4924-ab88-f6abe0dc58d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Create a dictionary to hold the selected features from each method\n",
    "features_select = {\n",
    "    'pearson': pearson_selected_features,\n",
    "    'spearman': spearman_selected_features,\n",
    "    'kendall': kendall_selected_features,\n",
    "    'mutual_info': MIM_selected_features,\n",
    "    'lasso': Lasso_selected_features,\n",
    "    'elastic_net': Elastic_selected_features,\n",
    "    'rfe': RFE_selected_features,\n",
    "    'pca': PCA_selected_features,\n",
    "    'boruta': Boruta_selected_features,\n",
    "    'random_forest': RF_selected_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac594fe6-26d9-4f5b-a859-da56810e67d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, auc\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize an empty list to store the ROC curves\n",
    "roc_curves_list = []\n",
    "\n",
    "le = LabelEncoder()\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Neural Network': MLPClassifier(verbose=False),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Support Vector Machine': SVC(probability=True),\n",
    "    'XGBoost': XGBClassifier(),\n",
    "    'Ridge': CalibratedClassifierCV(RidgeClassifier()) \n",
    "}\n",
    "\n",
    "hyperparameters = {\n",
    "    'Logistic Regression': {'penalty': ['l1', 'l2'], 'C': [0.1, 1, 10]},\n",
    "    'Naive Bayes': {},\n",
    "    'KNN': {'n_neighbors': [5, 10, 15], 'weights': ['uniform', 'distance']},\n",
    "    'Neural Network': {'hidden_layer_sizes': [(10,), (20,), (30,)], 'alpha': [0.0001, 0.01, 0.1]},\n",
    "    'Random Forest': {'n_estimators': [50, 100, 200, 400 ], 'max_depth': [5, 10, None], 'min_samples_split': [2, 10]},\n",
    "    'Support Vector Machine': {'C': [0.1, 1, 10], 'gamma': [0.1, 0.01, 0.001]},\n",
    "    'XGBoost': {'n_estimators': [100, 200], 'learning_rate': [0.1, 0.2], 'max_depth': [3, 5]},\n",
    "    'Ridge': {}\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Record the ROC curves and feature importance\n",
    "roc_curves = {}\n",
    "feature_importance = {}\n",
    "\n",
    "for feature_set_name, feature_set in features_select.items():\n",
    "    roc_curves_per_feature_set = []\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_imputed[feature_set], le.fit_transform(train_imputed['outcome var']).astype(int), test_size=0.2, random_state=42)\n",
    "    X_test = test_imputed[feature_set]\n",
    "    y_test = le.transform(test_imputed['outcome var']).astype(int)\n",
    "\n",
    "    results_per_feature_set = []\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=42)\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        if hyperparameters[model_name]:\n",
    "            model = GridSearchCV(model, param_grid=hyperparameters[model_name], cv=cv, scoring=make_scorer(roc_auc_score, needs_proba=True))\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_train_labels = model.predict(X_train)\n",
    "        y_pred_val_labels = model.predict(X_val)\n",
    "        y_pred_test_labels = model.predict(X_test)\n",
    "\n",
    "        # Get the probability estimates\n",
    "        y_pred_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "        y_pred_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "        y_pred_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        precision_train = precision_score(y_train, y_pred_train_labels)\n",
    "        precision_val = precision_score(y_val, y_pred_val_labels)\n",
    "        precision_test = precision_score(y_test, y_pred_test_labels)\n",
    "\n",
    "        recall_train = recall_score(y_train, y_pred_train_labels)\n",
    "        recall_val = recall_score(y_val, y_pred_val_labels)\n",
    "        recall_test = recall_score(y_test, y_pred_test_labels)\n",
    "\n",
    "        f1_train = 2 * (precision_train * recall_train) / (precision_train + recall_train)\n",
    "        f1_val = 2 * (precision_val * recall_val) / (precision_val + recall_val)\n",
    "        f1_test = 2 * (precision_test * recall_test) / (precision_test + recall_test)\n",
    "\n",
    "               # for train data\n",
    "        fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_train_proba)\n",
    "        roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "        # for validation data\n",
    "        fpr_val, tpr_val, _ = roc_curve(y_val, y_pred_val_proba)\n",
    "        roc_auc_val = auc(fpr_val, tpr_val)\n",
    "\n",
    "        # for test data\n",
    "        fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_test_proba)\n",
    "        roc_auc_test = auc(fpr_test, tpr_test)\n",
    "        \n",
    "        cm_train = confusion_matrix(y_train, y_pred_train_labels)\n",
    "        cm_val = confusion_matrix(y_val, y_pred_val_labels)\n",
    "        cm_test = confusion_matrix(y_test, y_pred_test_labels)\n",
    "\n",
    "        tp_train, fp_train, tn_train, fn_train = cm_train.ravel()\n",
    "        tp_val, fp_val, tn_val, fn_val = cm_val.ravel()\n",
    "        tp_test, fp_test, tn_test, fn_test = cm_test.ravel()\n",
    "\n",
    "        sensitivity_train = tp_train / (tp_train + fn_train)\n",
    "        sensitivity_val = tp_val / (tp_val + fn_val)\n",
    "        sensitivity_test = tp_test / (tp_test + fn_test)\n",
    "\n",
    "        specificity_train = tn_train / (tn_train + fp_train)\n",
    "        specificity_val = tn_val / (tn_val + fp_val)\n",
    "        specificity_test = tn_test / (tn_test + fp_test)\n",
    "        \n",
    "           # Get ROC curves\n",
    "        fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_train_proba)\n",
    "        fpr_val, tpr_val, _ = roc_curve(y_val, y_pred_val_proba)\n",
    "        fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_test_proba)\n",
    "        \n",
    "        roc_curves_per_feature_set.append({\n",
    "            'model': model_name,\n",
    "            'train': {'fpr': fpr_train, 'tpr': tpr_train},\n",
    "            'val': {'fpr': fpr_val, 'tpr': tpr_val},\n",
    "            'test': {'fpr': fpr_test, 'tpr': tpr_test}\n",
    "        })\n",
    "        \n",
    "        \n",
    "\n",
    "        # Append the results to the lists\n",
    "        results_per_feature_set.append({\n",
    "            'model': model_name,\n",
    "            'roc_auc_train': roc_auc_train,\n",
    "            'roc_auc_val': roc_auc_val,\n",
    "            'roc_auc_test': roc_auc_test,\n",
    "            'f1_train': f1_train,\n",
    "            'f1_val': f1_val,\n",
    "            'f1_test': f1_test,\n",
    "            'recall_train': recall_train,\n",
    "            'recall_val': recall_val,\n",
    "            'recall_test': recall_test,\n",
    "            'precision_train': precision_train,\n",
    "            'precision_val': precision_val,\n",
    "            'precision_test': precision_test,\n",
    "            'sensitivity_train': sensitivity_train,\n",
    "            'sensitivity_val': sensitivity_val,\n",
    "            'sensitivity_test': sensitivity_test,\n",
    "            'specificity_train': specificity_train,\n",
    "            'specificity_val': specificity_val,\n",
    "            'specificity_test': specificity_test,\n",
    "        })\n",
    "\n",
    "    results.append({\n",
    "        'feature_set': feature_set_name,\n",
    "        'results': results_per_feature_set\n",
    "    })\n",
    "    roc_curves_list.append({\n",
    "        'feature_set': feature_set_name,\n",
    "        'roc_curves': roc_curves_per_feature_set\n",
    "    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d1bbff-9f72-4837-b444-e8436670b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists for each metric\n",
    "train_auc = []\n",
    "val_auc = []\n",
    "test_auc = []\n",
    "\n",
    "train_f1 = []\n",
    "val_f1 = []\n",
    "test_f1 = []\n",
    "\n",
    "train_recall = []\n",
    "val_recall = []\n",
    "test_recall = []\n",
    "\n",
    "train_precision = []\n",
    "val_precision = []\n",
    "test_precision = []\n",
    "\n",
    "train_sensitivity = []\n",
    "val_sensitivity = []\n",
    "test_sensitivity = []\n",
    "\n",
    "train_specificity = []\n",
    "val_specificity = []\n",
    "test_specificity = []\n",
    "\n",
    "# Iterate over the results list\n",
    "for result in results:\n",
    "    for model_result in result['results']:\n",
    "        # Append the metrics to the respective lists\n",
    "        train_auc.append(model_result['roc_auc_train'])\n",
    "        val_auc.append(model_result['roc_auc_val'])\n",
    "        test_auc.append(model_result['roc_auc_test'])\n",
    "\n",
    "        train_f1.append(model_result['f1_train'])\n",
    "        val_f1.append(model_result['f1_val'])\n",
    "        test_f1.append(model_result['f1_test'])\n",
    "\n",
    "        train_recall.append(model_result['recall_train'])\n",
    "        val_recall.append(model_result['recall_val'])\n",
    "        test_recall.append(model_result['recall_test'])\n",
    "\n",
    "        train_precision.append(model_result['precision_train'])\n",
    "        val_precision.append(model_result['precision_val'])\n",
    "        test_precision.append(model_result['precision_test'])\n",
    "\n",
    "        train_sensitivity.append(model_result['sensitivity_train'])\n",
    "        val_sensitivity.append(model_result['sensitivity_val'])\n",
    "        test_sensitivity.append(model_result['sensitivity_test'])\n",
    "\n",
    "        train_specificity.append(model_result['specificity_train'])\n",
    "        val_specificity.append(model_result['specificity_val'])\n",
    "        test_specificity.append(model_result['specificity_test'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea96fb6-7fac-4f8a-b49b-5b5e0ab4580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize lists to store the results\n",
    "roc_auc_test_results = []\n",
    "f1_test_results = []\n",
    "\n",
    "# Iterate over the results\n",
    "for result in results:\n",
    "    for model_result in result['results']:\n",
    "        # Append the test ROC AUC and F1-scores to the respective lists\n",
    "        roc_auc_test_results.append(model_result['roc_auc_val'])\n",
    "        f1_test_results.append(model_result['f1_val'])\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "roc_auc_test_matrix = np.array(roc_auc_test_results).reshape(len(models), len(features_select))\n",
    "f1_test_matrix = np.array(f1_test_results).reshape(len(models), len(features_select))\n",
    "\n",
    "# Combine the ROC AUC and F1-score matrices\n",
    "combined_matrix = roc_auc_test_matrix + f1_test_matrix\n",
    "\n",
    "# Find the indices of the maximum value in the combined matrix\n",
    "best_model_index, best_feature_index = np.unravel_index(np.argmax(combined_matrix), combined_matrix.shape)\n",
    "\n",
    "# Get the corresponding best model and feature selection technique\n",
    "best_model = list(models.keys())[best_model_index]\n",
    "best_feature = list(features_select.keys())[best_feature_index]\n",
    "\n",
    "# Print the best model x feature selection combination\n",
    "print(\"Best Model x Feature Selection Combination based on AUC and F1-score:\")\n",
    "print(\"Model:\", best_model)\n",
    "print(\"Feature Selection Technique:\", best_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e210d6-96e7-464f-93f8-d84f7a710280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the models and feature selection techniques\n",
    "models = [roc_curve['model'] for roc_curve in results[0]['results']]  # Assuming the same models are used for each feature selection method\n",
    "feature_select_methods = [result['feature_set'] for result in results]\n",
    "\n",
    "# Initialize lists to store the results\n",
    "results_dict = {\n",
    "    'roc_auc_train': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'roc_auc_val': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'roc_auc_test': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'f1_train': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'f1_val': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'f1_test': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'precision_train': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'precision_val': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'precision_test': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'recall_train': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'recall_val': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'recall_test': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'sensitivity_train': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'sensitivity_val': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'sensitivity_test': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'specificity_train': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'specificity_val': np.zeros((len(models), len(feature_select_methods))),\n",
    "    'specificity_test': np.zeros((len(models), len(feature_select_methods))),\n",
    "}\n",
    "\n",
    "# Iterate over the results\n",
    "for i, result in enumerate(results):\n",
    "    for j, model_result in enumerate(result['results']):\n",
    "        # Store the metrics in the respective arrays\n",
    "        for metric in results_dict.keys():\n",
    "            results_dict[metric][j, i] = model_result[metric]  # Transpose the matrix by swapping i and j\n",
    "\n",
    "# Create the subplots grid\n",
    "fig, axes = plt.subplots(nrows=6, ncols=3, figsize=(40, 40))\n",
    "\n",
    "# Flatten the axes to iterate over them\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Generate the heatmaps\n",
    "for (metric, matrix), ax in zip(results_dict.items(), axes):\n",
    "    sns.heatmap(matrix, annot=True, cmap='coolwarm', fmt=\".2f\", ax=ax, cbar=False)\n",
    "    ax.set_title(metric.replace('_', ' ').title())\n",
    "    ax.set_ylabel(\"Model\")  # Swap x and y labels\n",
    "    ax.set_xlabel(\"Feature Selection Technique\")  # Swap x and y labels\n",
    "    ax.set_yticklabels(models, rotation=0)  # Swap x and y tick labels\n",
    "    ax.set_xticklabels(feature_select_methods, rotation=90)  # Swap x and y tick labels\n",
    "    ax.tick_params(axis='both', which='both', length=0)  # Remove tick marks\n",
    "    ax.set_aspect('equal')  # Ensure equal aspect ratio for each subplot\n",
    "\n",
    "    # Adjust font size for better readability\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), fontsize=14)\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), fontsize=14)\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540c246e-fdb4-43a6-abef-b4ea89451ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a dictionary to hold the selected features from each method\n",
    "features_select = {\n",
    "    'pearson': pearson_selected_features,\n",
    "    'spearman': spearman_selected_features,\n",
    "    'kendall': kendall_selected_features,\n",
    "    'mutual_info': MIM_selected_features,\n",
    "    'lasso': Lasso_selected_features,\n",
    "    'elastic_net': Elastic_selected_features,\n",
    "    'rfe': RFE_selected_features,\n",
    "    'pca': PCA_selected_features,\n",
    "    'boruta': Boruta_selected_features,\n",
    "    'random_forest': RF_selected_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b55f8-ef69-4272-ade3-b632ea5dcf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features selected by Lasso\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, auc\n",
    "feature_set_name = 'lasso'  # Replace this with the correct key if it's different\n",
    "feature_set = features_select[feature_set_name]\n",
    "\n",
    "# Get the training, validation, and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_imputed[feature_set], le.fit_transform(train_imputed['Success.Y.N.Lost_Y']).astype(int), test_size=0.2, random_state=42)\n",
    "X_test = test_imputed[feature_set]\n",
    "y_test = le.transform(test_imputed['Success.Y.N.Lost_Y']).astype(int)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model_name = 'Logistic Regression'\n",
    "model = LogisticRegression()\n",
    "\n",
    "# If there are hyperparameters for Logistic Regression, perform hyperparameter tuning\n",
    "if hyperparameters[model_name]:\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=42)\n",
    "    model = GridSearchCV(model, param_grid=hyperparameters[model_name], cv=cv, scoring=make_scorer(roc_auc_score, needs_proba=True))\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the probabilities\n",
    "y_pred_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "y_pred_val_proba = model.predict_proba(X_val)[:, 1]\n",
    "y_pred_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate the ROC curves\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_train_proba)\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_pred_val_proba)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_test_proba)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_train, tpr_train, label='Train')\n",
    "plt.plot(fpr_val, tpr_val, label='Validation')\n",
    "plt.plot(fpr_test, tpr_test, label='Test')\n",
    "plt.title('ROC Curves')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Plot feature importances\n",
    "# Note: Logistic Regression does not directly provide feature importances. Instead, we can use the coefficients\n",
    "# from the logistic regression model as an indication of feature importance.\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feature_set, model.best_estimator_.coef_[0])\n",
    "plt.title('Feature Importances')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c102e6a5-cf1d-434f-98b8-56e2cf1ac642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "from scipy.stats import sem, t\n",
    "\n",
    "def compute_auc_ci(y_true, y_score, n_bootstraps=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute the AUC and its confidence interval using bootstrapping.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,)\n",
    "        True binary labels.\n",
    "    y_score : array-like of shape (n_samples,)\n",
    "        Target scores.\n",
    "    n_bootstraps : int, default=1000\n",
    "        Number of bootstraps.\n",
    "    alpha : float, default=0.05\n",
    "        Confidence level (e.g., 0.05 for a 95% confidence interval).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    auc : float\n",
    "        AUC score.\n",
    "    ci_lower : float\n",
    "        Lower bound of the confidence interval.\n",
    "    ci_upper : float\n",
    "        Upper bound of the confidence interval.\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_score), \"Lengths of y_true and y_score should be equal.\"\n",
    "    \n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    bootstrapped_scores = []\n",
    "    \n",
    "    for _ in range(n_bootstraps):\n",
    "        # Bootstrap by sampling with replacement\n",
    "        y_true_resampled, y_score_resampled = resample(y_true, y_score)\n",
    "        score = roc_auc_score(y_true_resampled, y_score_resampled)\n",
    "        bootstrapped_scores.append(score)\n",
    "    \n",
    "    sorted_scores = np.array(bootstrapped_scores)\n",
    "    sorted_scores.sort()\n",
    "    \n",
    "    # Compute the lower and upper bound of the confidence interval\n",
    "    confidence_lower = sorted_scores[int((alpha / 2.0) * n_bootstraps)]\n",
    "    confidence_upper = sorted_scores[int((1 - alpha / 2.0) * n_bootstraps)]\n",
    "    \n",
    "    return auc, confidence_lower, confidence_upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917d776c-cacd-4a1b-8588-c82768747859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the AUC and its CI for the train set\n",
    "auc_train, ci_lower_train, ci_upper_train = compute_auc_ci(y_train, y_pred_train_proba)\n",
    "\n",
    "# Compute the AUC and its CI for the validation set\n",
    "auc_val, ci_lower_val, ci_upper_val = compute_auc_ci(y_val, y_pred_val_proba)\n",
    "\n",
    "# Compute the AUC and its CI for the test set\n",
    "auc_test, ci_lower_test, ci_upper_test = compute_auc_ci(y_test, y_pred_test_proba)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr_train, tpr_train, label=f'Train (AUC = {auc_train:.2f}, CI = [{ci_lower_train:.2f}, {ci_upper_train:.2f}])')\n",
    "plt.plot(fpr_val, tpr_val, label=f'Validation (AUC = {auc_val:.2f}, CI = [{ci_lower_val:.2f}, {ci_upper_val:.2f}])')\n",
    "plt.plot(fpr_test, tpr_test, label=f'Test (AUC = {auc_test:.2f}, CI = [{ci_lower_test:.2f}, {ci_upper_test:.2f}])')\n",
    "plt.title('ROC Curves')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8db2f78-2fe7-4277-bf30-1f4539d8b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_metrics_table(y_true, y_pred, y_score, n_bootstraps=1000, alpha=0.05):\n",
    "    # Ensure input lengths match\n",
    "    assert len(y_true) == len(y_pred) == len(y_score), \"Input lengths should be equal.\"\n",
    "    \n",
    "    # Initialize an empty dictionary to hold your metrics\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    # Compute basic metrics\n",
    "    metrics_dict['Accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    metrics_dict['Balanced Accuracy'] = balanced_accuracy_score(y_true, y_pred)\n",
    "    metrics_dict['F1 Score'] = f1_score(y_true, y_pred)\n",
    "    metrics_dict['Sensitivity / Recall'] = recall_score(y_true, y_pred)\n",
    "    metrics_dict['Specificity'] = recall_score(y_true, y_pred, pos_label=0)  # Assuming 0 is the negative class\n",
    "    metrics_dict['PPV / Precision'] = precision_score(y_true, y_pred)\n",
    "    metrics_dict['Brier Score'] = brier_score_loss(y_true, y_score)\n",
    "    \n",
    "  \n",
    "    # ... (your existing code for other metrics)\n",
    "    \n",
    "    # Bootstrapping for Accuracy 95% CI\n",
    "    bootstrapped_acc_scores = []\n",
    "    for _ in range(n_bootstraps):\n",
    "        # Bootstrap by sampling with replacement\n",
    "        y_true_resampled, y_pred_resampled = resample(y_true, y_pred)\n",
    "        acc_score = accuracy_score(y_true_resampled, y_pred_resampled)\n",
    "        bootstrapped_acc_scores.append(acc_score)\n",
    "    \n",
    "    sorted_acc_scores = np.array(bootstrapped_acc_scores)\n",
    "    sorted_acc_scores.sort()\n",
    "    \n",
    "    # Compute the lower and upper bound of the confidence interval for Accuracy\n",
    "    acc_ci_lower = sorted_acc_scores[int((alpha / 2.0) * n_bootstraps)]\n",
    "    acc_ci_upper = sorted_acc_scores[int((1 - alpha / 2.0) * n_bootstraps)]\n",
    "    metrics_dict['Accuracy 95% CI'] = f'[{acc_ci_lower:.2f}, {acc_ci_upper:.2f}]'\n",
    "    \n",
    "    # ... (your existing code for other metrics)\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics_dict, index=['Value']).transpose()\n",
    "    \n",
    "    return metrics_df\n",
    "\n",
    "# Usage:\n",
    "# Assuming y_train, y_pred_train, y_score_train etc. are defined\n",
    "metrics_val = generate_metrics_table(y_val, y_pred_val_labels, y_pred_val_proba)\n",
    "metrics_test = generate_metrics_table(y_test,y_pred_test_labels, y_pred_test_proba )\n",
    "\n",
    "# Concatenate the DataFrames for a side-by-side comparison\n",
    "metrics_table = pd.concat([ metrics_val, metrics_test], axis=1, keys=['Validation', 'Test'])\n",
    "print(metrics_table)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
